# Flax Optimizers Benchmark

The goal of this repository is to provide users with a collection of benchmark to evaluate Flax optimizers.

We aim to:
- give some fast benchmarks to quickly evaluate new optimizers
- give classic benchmarks and plotting functions to help authors of deeplearning papers that want to publish a new optimizer

## Installation

You can install this librarie with:

```
pip install git+https://github.com/nestordemeure/flaxOptimizersBenchmark.git
```

## Usage

### Fast benchmarks

**TODO**

### Slow benchmarks

**TODO**

## Flax optimizers

- [flaxOptimizers](https://github.com/nestordemeure/flaxOptimizers) contains implementations of a large number of optimizers in Flax.
- [AdahessianJax](https://github.com/nestordemeure/AdaHessianJax) contains my implementation of the Adahessian second order optimizer in Flax.
- [Flax.optim](https://github.com/google/flax/tree/master/flax/optim) contains the official Flax optimizers.

